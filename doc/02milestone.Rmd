---
title: "Exploratory Data Analysis of Text Corpus"
author: "Ivan Lysiuchenko"
date: "September 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introductions

In this milestone report we present an overview and exploratory analysis of the text corpus
we use to build our predictive model. We briefly describe the loading and preprocessing step
in Section 1. In Section 2 we explore the distribution of
1-, 2-, 3- and 4-grams in the overall corpus. Section 3 is an overview of the three files
used to build the corpus (they represent extracts from blogs, news and tweets in English).
Then we draw some conclusions on the observed distributions and outline further steps
in building the predictive model.

This report is totally reproducible. You can find all the code 
in [this repository](https://github.com/ivanlis/yellow_dog_predictor). 
To work with corpora and n-grams, we use the quanteda R package.


## 1 Loading and feature extraction

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Set this to true if you want to build/rebuild tokens, DFMs etc.
rebuildFiles <- FALSE
```

An important problem arising when trying to load the corpora is the limited size of the main
memory. We've been performing the loading task on a PC with 8 GB RAM (actually less available to R).
As it is impossible to load all the three files into the memory and perform operations on them,
we divide each one into chunks. Our splitFiles() function is in charge of doing that.

Then we execute extractFeatures(). When it is finished, we have the document-feature matrices
(DFMs) of n-grams stored on the disk. They will be the base for our further research.

Pre-processing is done in several steps:

- Load a chunk file, create a quanteda corpus. Reshape it by sentence. 
- Tokenize the corpus into unigrams, only removing punctuation. Let's call these tokens "raw".
- For each n = 1..4 build n-gram tokens out of the raw tokens.
- Filter these tokens: remove profane words, remove tokens containing non-alphabetic characters
(except for the apostrophe and hyphen)
- Build a document-feature matrix (DFM) for the current corpus and current n.
- Merge the DFMs for all the chunks. This way we have 4 DFMs, one for each n-gram type.

We process all the unigrams and bigrams.
For 3- and 4-grams we keep nearly 60% of the most frequent features due to memory restrictions.

```{r warning=FALSE, message=FALSE, echo=FALSE}
source("../TextFileTools/splitFiles.R")
source("../nGram/extractFeatures.R")
if (rebuildFiles)
{
    splitFiles()
    
    # Build tokens and DFMs for each small file, store them on the disk
    extractFeatures(tokensDirectory = "../results",
                    matrixDirectory = "../results",
                    buildTokens = TRUE,
                    buildMatrices = TRUE)
    # Now, based on the tokens and partial DFMs, build overall DFMs
    # for 1-, 2-, 3- and 4-grams
    extractFeatures(tokensDirectory = "../results",
                    matrixDirectory = "../results",
                    buildTokens = FALSE,
                    buildMatrices = FALSE,
                    mergeMatrices = TRUE,
                    mergeFiltered = TRUE)

    file.copy(list.files(path = "../results", pattern = "generalDfm*", full.names = TRUE, recursive = FALSE), "../results/dfm")    
}
```


## 2 Distribution analysis of the overall corpus

In this section we'll explore how the frequencies of the features (1-, 2-, 3- and 4-grams) are distributed.
We use our loadFrequencies() function which reads a DFM from a file and
converts it into a vector of frequency values sorted in descendant order.

### Words

Let's take a look at words first.

```{r message=FALSE, echo=FALSE}
source("../nGram/loadFrequencies.R")
library(ggplot2)
library(scales)

freqs <- loadFrequencies("../results/dfm/generalDfm1.dat", "../results/dfm/freq1.dat")

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

sufficient50_1 <- sufficient50
sufficient90_1 <- sufficient90

dsize <- min(sufficient90 * 100 + 100, length(freqs))
g <- ggplot(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(1, dsize))), 
                                    mapping = aes(x = x, y = y, color = z)) + 
    geom_line() +
    geom_vline(data = data.frame(x = sufficient50, z = as.factor(1)), mapping = aes(xintercept = x, color = z)) + 
    geom_vline(data = data.frame(x = sufficient90, z = as.factor(1)), mapping = aes(xintercept = x, color = z)) +
    
    geom_text(mapping = aes(x = sufficient50_1, y = 5e-7, label = "50% 1-grams"), color = "black", 
              angle = 90, size = 4, vjust = -0.4) +
    geom_text(mapping = aes(x = sufficient90_1, y = 5e-7, label = "90% 1-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4) +
    
    labs(x = "num. feature", y = "frequency")
```

Our preprocessing step left `r length(freqs)` different words. 
All of them account for `r sum(freqs)` entries.
The most frequent words are the following:

```{r echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent words:

```{r echo=FALSE}
tail(freqs, 5)
```

It turns out that to cover 90% of all word usages we need to keep `r sufficient90` of the most frequent words.
50% is covered by `r sufficient50` of them. The most frequent word is `r names(freqs)[1]` with relative
frequency of `r freqs[1] / sum(freqs)`.

### Bigrams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm2.dat", "../results/dfm/freq2.dat")

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

sufficient50_2 <- sufficient50
sufficient90_2 <- sufficient90

#dsize <- length(freqs)
dsize <- min(sufficient90 + 100, length(freqs))
#g <- ggplot(data = data.frame(x = 1:dsize, y = log(freqs[1:dsize])), 
#                                    mapping = aes(x = x, y = y), color = "blue") + 
#    geom_line() +
#    geom_vline(xintercept = sufficient50) + geom_vline(xintercept = sufficient90)

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(2, dsize))), 
                                    mapping = aes(x = x, y = y, color = z)) +
    geom_vline(data = data.frame(x = sufficient50, z = as.factor(2)), mapping = aes(xintercept = x, color = z)) + 
    geom_vline(data = data.frame(x = sufficient90, z = as.factor(2)), mapping = aes(xintercept = x, color = z)) +
    
    geom_text(mapping = aes(x = sufficient50_2, y = 5e-7, label = "50% 2-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4) +
    geom_text(mapping = aes(x = sufficient90_2, y = 5e-7, label = "90% 2-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4)    
```

The number of different bigrams is `r length(freqs)`, the overall number of their entries is `r sum(freqs)`.
90% and 50% of the entries correspond to `r sufficient90` and `r sufficient50` most frequent bigrams respectively.
The most frequent bigram is `r names(freqs)[1]`, its relative frequency is `r freqs[1] / sum(freqs)`.
Other frequent bigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent bigrams:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Trigrams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm3.dat", "../results/dfm/freq3.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(3, dsize))), 
                                    mapping = aes(x = x, y = y, color = z))
```

At the preprocessing step we filtered out the less frequent trigrams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The highest relative frequency of `r freqs[1] / sum(freqs)` is achieved at the trigram `r names(freqs)[1]`.
The most frequent trigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Among our fitered trigrams, these are the less frequent:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Four-grams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm4.dat", "../results/dfm/freq4.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(4, dsize))), 
                                    mapping = aes(x = x, y = y, color = z))
```

At the preprocessing step we filtered out the less frequent 4-grams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The relative frequency of the most seen 4-gram is `r freqs[1] / sum(freqs)`.
The most frequent 4-grams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

These are the less frequent among our fitered 4-grams:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Empirical frequency distributions

Let's show the frequencies of the words, bigrams, trigrams and 4-grams on the same graph,
using a logarithmic (log-log) scale.

```{r echo=FALSE}
rm(freqs)
g <- g + 
    scale_y_continuous(trans = "log", 
                       breaks = c(5e-7, 5e-5, 5e-3),
                       labels = c(5e-7, 5e-5, 5e-3)) + 
    scale_x_continuous(trans = "log", 
                       breaks = c(1, 1000, 1000000),
                       labels = c(1, 1000, 1000000)) +
    scale_color_discrete(name = "Features", 
                         #breaks = as.factor(c(1, 2)),
                         labels = c("1-gram", "2-gram", "3-gram", "4-gram"))
g
```

It may be interesting to take a look at the random variable whose values are all possible values of relative frequency (from 0 to 1).
We estimate the probability of a relative frequency value as the number of the features occurring with this relative frequency divided by
the overall number of features.

```{r eval=TRUE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm1.dat", "../results/dfm/freq1.dat")
g <- ggplot() + geom_boxplot(data = data.frame(x = rep(1, length(freqs)), y = freqs / sum(freqs)),  mapping = aes(x = x, y = y, group = x))
```

Here is the summary related to unigrams:

```{r echo=FALSE}
summary(freqs / sum(freqs))
```

The mean of the relative frequency for unigrams is `r mean(freqs / sum(freqs))`, whereas its standard deviation is `r sd(freqs / sum(freqs))`.

```{r echo=FALSE}
rm(freqs)

# ... for 2-grams
freqs <- loadFrequencies("../results/dfm/generalDfm2.dat", "../results/dfm/freq2.dat")
g <- g + geom_boxplot(data = data.frame(x = rep(2, length(freqs)), y = freqs / sum(freqs)),  mapping = aes(x = x, y = y, group = x)) + scale_y_continuous(trans = "log") + labs(x = "n-gram type", y = "relative frequency")

```

Here is the summary related to bigrams:

```{r echo=FALSE}
summary(freqs / sum(freqs))
```

The mean of the relative frequency for bigrams is `r mean(freqs / sum(freqs))`, whereas its standard deviation is `r sd(freqs / sum(freqs))`.
We can see that the mean for unigrams is higher, and the standard deviation is bigger as well. It means that on average
there are more important high frequency features than in the case of bigrams. Also there is more dispersion of frequency values
for unigrams in comparison with bigrams. The majority of words have low frequency, although there is a set of very high-frequency
words. 2-grams show a similar pattern, but tend to have lower on average, less dispersed frequencies.

```{r echo=FALSE}
rm(freqs)

##freqs <- loadFrequencies("../results/dfm/generalDfm3.dat", "../results/dfm/freq3.dat")
##dfFreq <- as.data.frame(table(freqs))
##g <- g + geom_violin(data = data.frame(x = rep(3, nrow(dfFreq)), y = log(as.numeric(dfFreq$Freq))),  mapping = aes(x = x, y ##= y, group = x)) + geom_violin()

##rm(freqs)
##rm(dfFreq)
```

In the following box plot it can be seen how the relative frequency distributions are different between unigrams
and bigrams:

```{r echo=FALSE}
g
```

Both distributions are right skewed (towards higher frequencies). This efect is stronger in the case of unigrams: the right tail is 'heavier'.


## Files overview

### Lines count and length

To see what the lines of every file look like we use a simple function called computeLineStats(). 
The minimum, maximum, average line length as well as line counts are as follows:

```{r message=FALSE, warning=FALSE, echo=FALSE}
source("../TextFileTools/computeLineStats.R")
fileNames <- c(
    "../materials/datasets/final/en_US/en_US.blogs.txt",
    "../materials/datasets/final/en_US/en_US.news.txt",
    "../materials/datasets/final/en_US/en_US.twitter.txt"
)
fileDescr <- c("Blogs", "News", "Twitter")
linestats <- computeLineStats(fileNames, fileDescr)
linestats
```

We can see that the line length average is similar in the blogs and news texts included in
the corpora. On the opposite, no line belonging to the tweets exceeds 140 characters.

### Vocabulary and n-grams

Let's explore n-gram tokens for the three files our corpus consists of.
We analyze the tokens computed after dividing the correspondent corpus into sentences.
Tokens containing punctuation, numbers and profane words are deleted.


```{r echo = FALSE, message=FALSE}
vocab <- data.frame()
frequentNgrams <- data.frame()

if (!file.exists("../results/vocab.csv") || !file.exists("../results/frequentNgrams.csv"))
{
    # here we will store the number of types and number of tokens
    vocab <- data.frame(docName = rep(c("Blogs", "News", "Tweets"), 4),
                        nGram = c(rep(1, 3), rep(2, 3), rep(3, 3), rep(4, 3)), nTypes = rep(0, 12), nTokens = rep(0, 12))
    
    # here we will store the 10 most frequent features
    frequentNgrams <- data.frame(docName = rep(c(rep("Blogs", 10), rep("News", 10), rep("Tweets", 10)), 4),
                                 nGram = rep(c(rep(1, 30), rep(2, 30), rep(3, 30), rep(4,30))),
                                 entry = rep("", 120), rank = rep(0, 120), stringsAsFactors = FALSE
                                 )
    
    freqs <- dfmsToFreq("../results", "../results/freq1_blogs.dat", 1, 0:3)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 1, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 1, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 1, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 1, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq1_news.dat", 1, 4:8)
    vocab[vocab$docName == "News" & vocab$nGram == 1, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "News" & vocab$nGram == 1, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 1, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 1, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq1_tweets.dat", 1, 9:18)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 1, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 1, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 1, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 1, "rank"] <- 1:10
    
    
    freqs <- dfmsToFreq("../results", "../results/freq2_blogs.dat", 2, 0:3)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 2, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 2, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 2, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 2, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq2_news.dat", 2, 4:8)
    vocab[vocab$docName == "News" & vocab$nGram == 2, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "News" & vocab$nGram == 2, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 2, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 2, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq2_tweets.dat", 2, 9:18)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 2, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 2, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 2, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 2, "rank"] <- 1:10
    
    
    freqs <- dfmsToFreq("../results", "../results/freq3_blogs.dat", 3, 0:3)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 3, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 3, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 3, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 3, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq3_news.dat", 3, 4:8)
    vocab[vocab$docName == "News" & vocab$nGram == 3, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "News" & vocab$nGram == 3, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 3, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 3, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq3_tweets.dat", 3, 9:18)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 3, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 3, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 3, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 3, "rank"] <- 1:10
    
    
    freqs <- dfmsToFreq("../results", "../results/freq4_blogs.dat", 4, 0:3)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 4, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Blogs" & vocab$nGram == 4, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 4, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Blogs" & frequentNgrams$nGram  == 4, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq4_news.dat", 4, 4:8)
    vocab[vocab$docName == "News" & vocab$nGram == 4, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "News" & vocab$nGram == 4, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 4, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "News" & frequentNgrams$nGram  == 4, "rank"] <- 1:10
    freqs <- dfmsToFreq("../results", "../results/freq4_tweets.dat", 4, 9:18)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 4, "nTypes"] <- length(freqs)
    vocab[vocab$docName == "Tweets" & vocab$nGram == 4, "nTokens"] <- sum(freqs)
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 4, "entry"] <- names(freqs)[1:10]
    frequentNgrams[frequentNgrams$docName == "Tweets" & frequentNgrams$nGram  == 4, "rank"] <- 1:10
    
    rm(freqs)
} else
{
    vocab <- read.csv("../results/vocab.csv")
    frequentNgrams <- read.csv("../results/frequentNgrams.csv")
}

library(tidyr)
frequentNgrams <- frequentNgrams %>% spread(nGram, entry)
```

### Blogs

The numbers of types (unique features) and tokens (their entries) for the blog corpus is the following:

```{r echo=FALSE}
vocab[vocab$docName == "Blogs", c("nGram", "nTypes", "nTokens")]
```

The most frequent n-grams for this corpus:

```{r echo=FALSE}
frequentNgrams[frequentNgrams$docName == "Blogs", -which(names(frequentNgrams) %in% c("docName", "rank"))]
```

### News

Types and tokens for the news corpus:

```{r echo=FALSE}
vocab[vocab$docName == "News", c("nGram", "nTypes", "nTokens")]
```

The most frequent n-grams:

```{r echo=FALSE}
frequentNgrams[frequentNgrams$docName == "News", -which(names(frequentNgrams) %in% c("docName", "rank"))]
```

### Tweets

Types and tokens for the news corpus:

```{r echo=FALSE}
vocab[vocab$docName == "Tweets", c("nGram", "nTypes", "nTokens")]
```

The most frequent n-grams:

```{r echo=FALSE}
frequentNgrams[frequentNgrams$docName == "Tweets", -which(names(frequentNgrams) %in% c("docName", "rank"))]
```

Having a look at the most frequent n-grams suggests that the Blogs and News corpora have more similarities between
them as opposed to the language of tweets. As for unigrams, the lists are quite similar between all the corpora,
except for the use of pronouns ('I', 'you'). At the bigram level we see more differences, When it comes to the 
10 most frequent trigrams, we observe clear divergence between the Tweets on one side and Blogs and News on the other.
The 4-grams reveal each corpus language nature, once more emphasizing the peculiarities of the 'Twitter language'.
 

## Conclusion and plans for development

Our exploration of feature frequencies shows that the most frequent words 'dominate' over the rest to a greater extent than the most 
frequent bigrams do so over the rest of bigrams. It's true for higher order n-grams as well. The greater n, the more 'spread' the distribution
of frequencies is among features.
In practical terms, it's easy to select some small number of high-frequency words covering the corpus almost entirely. The bigger n of
the n-gram, the harder this task gets.

Having a look at the most frequent n-grams, we notice heterogeneity in our corpus, especially regarding the tweets
language. It should be helpful, because more speech genres are covered.

The next steps to perform in the development of our predictive model are the following:

- Decide on the smoothing/backoff method to treat unknown features and combine the use of higher and lower order n-grams;
- Store the learned n-grams in a database for their fast extraction. This database has to be small enough to be loaded to the main memory. Here we'll have to limit the number of features using the findings of this exploratory analysis;
- Develop the scheme of model evaluation on a test data set;
- Develop the functions predicting word(s) based on the user input.
