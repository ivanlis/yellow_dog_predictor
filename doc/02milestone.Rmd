---
title: "Exploratory Data Analysis of Text Corpus"
author: "Ivan Lysiuchenko"
date: "September 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this milestone report we present an overview and exploratory analysis of the three files
we use to build our predictive model. These files represent text corpora extracted from
blogs, news and tweets in English.

## Files overview

To see what the lines of every file look like we use a simple function that can be found
in our Github repository. The minimum, maximum, average line length as well as line counts
are as follows:

```{r message=FALSE, warning=FALSE}
source("../TextFileTools/computeLineStats.R")
fileNames <- c(
    "../materials/datasets/final/en_US/en_US.blogs.txt",
    "../materials/datasets/final/en_US/en_US.news.txt",
    "../materials/datasets/final/en_US/en_US.twitter.txt"
)
fileDescr <- c("Blogs", "News", "Twitter")
linestats <- computeLineStats(fileNames, fileDescr)
linestats
```

We can see that the line length average is similar in the blogs and news texts included in
the corpora. On the opposite, no line belonging to the tweets exceeds 140 characters.

## Loading and feature extraction

```{r echo=FALSE, warning=FALSE, message=FALSE}
rebuildFiles <- FALSE
```

An important problem arising when trying to load the corpora is the limited size of the main
memory. We've been performing the loading task on a PC with 8 GB RAM (actually less available to R).
As it is impossible to load all the three files into the memory and perform operations on them,
we divide each one into chunks. Our splitFiles() function is in charge of doing that.

Then we execute the extractFeatures(). When we are finished, we have the document-feature matrices
(DFMs) of n-grams stored on the disk. They will be the base of our further research.

```{r warning=FALSE, message=FALSE}
source("../TextFileTools/splitFiles.R")
source("../nGram/extractFeatures.R")
if (rebuildFiles)
{
    splitFiles()
    
    # Build tokens and DFMs for each small file, store them on the disk
    extractFeatures(tokensDirectory = "../results",
                    matrixDirectory = "../results",
                    buildTokens = TRUE,
                    buildMatrices = TRUE)
    # Now, based on the tokens and partial DFMs, build overall DFMs
    # for 1-, 2-, 3- and 4-grams
    extractFeatures(tokensDirectory = "../results",
                    matrixDirectory = "../results",
                    buildTokens = FALSE,
                    buildMatrices = FALSE,
                    mergeMatrices = TRUE,
                    mergeFiltered = TRUE)
}
```

TODO: explain the steps performed by extractFeatures()

## Distribution analysis of the overall corpus

In this section we'll explore how the frequencies of the features (1-, 2-, 3- and 4-grams) are distributed.
We use our loadFrequencies() function which reads a DFM from a file and
converts it into a vector of frequency values sorted in descendant order.

### Words

Let's take a look at words first.

```{r message=FALSE, echo=FALSE}
source("../nGram/loadFrequencies.R")
library(ggplot2)
library(scales)

freqs <- loadFrequencies("../results/dfm/generalDfm1.dat", "../results/dfm/freq1.dat")

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

sufficient50_1 <- sufficient50
sufficient90_1 <- sufficient90

dsize <- min(sufficient90 * 100 + 100, length(freqs))
g <- ggplot(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(1, dsize))), 
                                    mapping = aes(x = x, y = y, color = z)) + 
    geom_line() +
    geom_vline(data = data.frame(x = sufficient50, z = as.factor(1)), mapping = aes(xintercept = x, color = z)) + 
    geom_vline(data = data.frame(x = sufficient90, z = as.factor(1)), mapping = aes(xintercept = x, color = z)) +
    
    geom_text(mapping = aes(x = sufficient50_1, y = 5e-7, label = "50% 1-grams"), color = "black", 
              angle = 90, size = 4, vjust = -0.4) +
    geom_text(mapping = aes(x = sufficient90_1, y = 5e-7, label = "90% 1-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4) +
    
    labs(x = "num. feature", y = "frequency")
```

Our preprocessing step detected `r length(freqs)` different words. 
All of them account for `r sum(freqs)` entries.
The most frequent words are the following:

```{r echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent words:

```{r echo=FALSE}
tail(freqs, 5)
```

It turns out that to cover 90% of all word usages we need to keep `r sufficient90` of the most frequent words.
50% is covered by `r sufficient50` of them. The most frequent word is `r names(freqs)[1]` with relative
frequency of `r freqs[1] / sum(freqs)`.

### Bigrams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm2.dat", "../results/dfm/freq2.dat")

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

sufficient50_2 <- sufficient50
sufficient90_2 <- sufficient90

#dsize <- length(freqs)
dsize <- min(sufficient90 + 100, length(freqs))
#g <- ggplot(data = data.frame(x = 1:dsize, y = log(freqs[1:dsize])), 
#                                    mapping = aes(x = x, y = y), color = "blue") + 
#    geom_line() +
#    geom_vline(xintercept = sufficient50) + geom_vline(xintercept = sufficient90)

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(2, dsize))), 
                                    mapping = aes(x = x, y = y, color = z)) +
    geom_vline(data = data.frame(x = sufficient50, z = as.factor(2)), mapping = aes(xintercept = x, color = z)) + 
    geom_vline(data = data.frame(x = sufficient90, z = as.factor(2)), mapping = aes(xintercept = x, color = z)) +
    
    geom_text(mapping = aes(x = sufficient50_2, y = 5e-7, label = "50% 2-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4) +
    geom_text(mapping = aes(x = sufficient90_2, y = 5e-7, label = "90% 2-grams"), color = "black",
              angle = 90, size = 4, vjust = -0.4)    
```

The number of different bigrams is `r length(freqs)`, the overall number of their entries is `r sum(freqs)`.
90% and 50% of the entries correspond to `r sufficient90` and `r sufficient50` most frequent bigrams.
The most frequent bigram is `r names(freqs)[1]`, its relative frequency is `r freqs[1] / sum(freqs)`.
Other frequent bigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent bigrams:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Trigrams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm3.dat", "../results/dfm/freq3.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(3, dsize))), 
                                    mapping = aes(x = x, y = y, color = z))
```

At the preprocessing step we filtered out the less frequent trigrams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The highest relative frequency of `r freqs[1] / sum(freqs)` is achieved at the trigram `r names(freqs)[1]`.
The most frequent trigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Among our fitered trigrams, these are the less frequent:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Four-grams

```{r message=FALSE, echo=FALSE, eval=TRUE}
freqs <- loadFrequencies("../results/dfm/generalDfm4.dat", "../results/dfm/freq4.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = 1:dsize, y = freqs[1:dsize] / sum(freqs), z = as.factor(rep(4, dsize))), 
                                    mapping = aes(x = x, y = y, color = z))
```

At the preprocessing step we filtered out the less frequent 4-grams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The relative frequency of the most seen 4-gram is `r freqs[1] / sum(freqs)`.
The most frequent 4-grams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Among our fitered 4-grams, these are the less frequent:

```{r message=FALSE, echo=FALSE}
tail(freqs, 5)
```

### Empirical frequency distributions

Let's show the frequencies of the words, bigrams, trigrams and 4-grams on the same graph,
using a logarithmic (log-log) scale.

```{r echo=FALSE}
rm(freqs)
g <- g + 
    scale_y_continuous(trans = "log", 
                       breaks = c(5e-7, 5e-5, 5e-3),
                       labels = c(5e-7, 5e-5, 5e-3)) + 
    scale_x_continuous(trans = "log", 
                       breaks = c(1, 1000, 1000000),
                       labels = c(1, 1000, 1000000)) +
    scale_color_discrete(name = "Features", 
                         #breaks = as.factor(c(1, 2)),
                         labels = c("1-gram", "2-gram", "3-gram", "4-gram"))
g
```

It may be interesting to take a look at how the numbers of words and bigrams are distributed in function of their
relative frequency.

```{r eval=TRUE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm1.dat", "../results/dfm/freq1.dat")
g <- ggplot() + geom_boxplot(data = data.frame(x = rep(1, length(freqs)), y = freqs / sum(freqs)),  mapping = aes(x = x, y = y, group = x))
```

Here is the summary related to unigrams:

```{r echo=FALSE}
summary(freqs)
```

```{r echo=FALSE}
rm(freqs)

# ... for 2-grams
freqs <- loadFrequencies("../results/dfm/generalDfm2.dat", "../results/dfm/freq2.dat")
g <- g + geom_boxplot(data = data.frame(x = rep(2, length(freqs)), y = freqs / sum(freqs)),  mapping = aes(x = x, y = y, group = x)) + scale_y_continuous(trans = "log") + labs(x = "n-gram type", y = "relative frequency")

```

Here is the summary related to bigrams:

```{r echo=FALSE}
summary(freqs)
```

```{r echo=FALSE}
rm(freqs)

##freqs <- loadFrequencies("../results/dfm/generalDfm3.dat", "../results/dfm/freq3.dat")
##dfFreq <- as.data.frame(table(freqs))
##g <- g + geom_violin(data = data.frame(x = rep(3, nrow(dfFreq)), y = log(as.numeric(dfFreq$Freq))),  mapping = aes(x = x, y ##= y, group = x)) + geom_violin()

##rm(freqs)
##rm(dfFreq)
```

In the following box plot it can be seen how the relative frequency distributions are different between unigrams
and bigrams:

```{r echo=FALSE}
g
```
